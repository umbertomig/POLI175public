{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "243e38f9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# POLI 175 - Lecture 21\n",
    "\n",
    "## Supporting Vector Machines I"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4046a13",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b879690",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Supporting Vector Machines\n",
    "\n",
    "- The idea is to try to separate targets to be classified using a hyperplane on the feature space.\n",
    "\n",
    "- SVMs extend the use of a line by cleverly changing the data.\n",
    "\n",
    "- This approach is used for binary feature classification.\n",
    "\n",
    "- But it may be extended to multiclass classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde1bd55",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Maximal Margin Classifier\n",
    "\n",
    "- The idea is to search for a hyperplane that **separates** the targets.\n",
    "\n",
    "- A hyperplane is an *affine* subspace with a dimension one degree lower than the original space:\n",
    "    + Affine: Shifted by a constant (may not touch the origin)\n",
    "    + If $dim(X) = 2$ (plane), the hyperspace is a line.\n",
    "    + If $dim(X) = 3$ (cube), the hyperspace is a plane.\n",
    "    + And so on\n",
    "\n",
    "- It separates the space into two half-spaces."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639cf822",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Maximal Margin Classifier\n",
    "\n",
    "![img](../img/svm1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231d14c8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Maximal Margin Classifier\n",
    "\n",
    "![img](../img/svm2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731da624",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Maximal Margin Classifier\n",
    "\n",
    "Let $n$ training observations, $x_1, \\cdots, x_n$ and a vector of labels $y_1, \\cdots, y_n \\in \\{-1, 1\\}$. The maximal margin classifier consists in:\n",
    "\n",
    "\\begin{align} \n",
    "\\max_{\\beta_j, M} M & \\\\\n",
    "\\text{ subject to (1)} & \\quad \\sum_j \\beta_j^2 = 1 \\\\ \n",
    "\\text{(2)} & \\quad y_i(\\beta_0 + \\beta_1 x_{i1} + \\cdots + \\beta_p x_{ip}) \\geq M \n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8591f613",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Maximal Margin Classifier\n",
    "\n",
    "- Benefit: Extreme cases do not affect our classifier.\n",
    "    + The idea is that they are *supported* only by the observations close by.\n",
    "\n",
    "- Problems: Sensitivity and non-separability (next slide). So, *we could do better if we allowed our classifier to do poorly*. (what?)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39388f7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Maximal Margin Classifier\n",
    "\n",
    "- Problem 1: Changing very little in the data can lead to a completely different classification region.\n",
    "\n",
    "![img](../img/svm3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deaf8fcb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Maximal Margin Classifier\n",
    "\n",
    "- Problem 2: When not separable, impossible to find $M$.\n",
    "\n",
    "![img](../img/svm4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed46502",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Support Vector Classifier\n",
    "\n",
    "- Solves this problem by allowing some wrongly classified observations.\n",
    "\n",
    "![img](../img/svm5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60396a32",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Support Vector Classifier\n",
    "\n",
    "Let $n$ training observations, $x_1, \\cdots, x_n$ and a vector of labels $y_1, \\cdots, y_n \\in \\{-1, 1\\}$. Also, let $C$ be a measure of tolerance for wrong classifications. The support vector classifier consists in:\n",
    "\n",
    "\\begin{align} \n",
    "\\max_{\\beta_j, \\varepsilon_i M} M & \\\\\n",
    "\\text{ subject to (1)} & \\quad \\sum_j \\beta_j^2 = 1 \\\\ \n",
    "\\text{(2)} & \\ \\quad y_i(\\beta_0 + \\beta_1 x_{i1} + \\cdots + \\beta_p x_{ip}) \\geq M (1 - \\varepsilon_i) \\\\\n",
    "\\text{(3)} & \\ \\quad \\varepsilon_i \\geq 0 \\\\\n",
    "\\text{(4)} & \\quad \\sum_i \\varepsilon_i \\leq C\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2822bcf",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Support Vector Classifier\n",
    "\n",
    "![img](../img/svm6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6033e7aa",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Support Vector Classifier\n",
    "\n",
    "Benefits:\n",
    "\n",
    "1. $C$ indexes how much we tolerate wrong predictions.\n",
    "    + it is effectively the number of observations that we accept classify wrong.\n",
    "\n",
    "2. If, for a given $i$, $\\varepsilon_i = 0$, then the $i$ is correctly classified.\n",
    "\n",
    "3. If, for a given $i$, $\\varepsilon_i > 0$ (in this case, $\\varepsilon_i = 1$), then the $i$ is **incorrectly** classified."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101c58bb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Support Vector Classifier\n",
    "\n",
    "Benefits:\n",
    "\n",
    "4. **Important**: Only observations on the wrong side affect the classifier!\n",
    "\n",
    "- This is important and different from all other classifiers:\n",
    "    + LDA: Depends on the mean of all observations within one class.\n",
    "    + Logistic Regression: Also good, but still weakly (in fact, very weakly) affected by observations far from the decision boundary.\n",
    "\n",
    "- Support Vector Classifier: Only affected by cases around the supporting vector!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716eec95",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Support Vector Classifier\n",
    "\n",
    "Negative:\n",
    "\n",
    "1. Linear assumption means that there will always be datasets with poor classification.\n",
    "\n",
    "![img](../img/svm7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24850020",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Support Vector Machines\n",
    "\n",
    "- Now, let us drop the linear assumption.\n",
    "\n",
    "- To do so, we note that the restriction (2) in the problem above in:\n",
    "\n",
    "$$ \\quad y_i(\\beta_0 + \\beta_1 x_{i1} + \\cdots + \\beta_p x_{ip}) \\geq M (1 - \\varepsilon_i) $$\n",
    "\n",
    "- And this is the hyperplane:\n",
    "\n",
    "$$ \\beta_0 + \\beta_1 x_{i1} + \\cdots + \\beta_p x_{ip} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83a2ec1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Support Vector Machines\n",
    "\n",
    "- But any hyperplane is composed of something called an inner product:\n",
    "\n",
    "$$ \\beta_0 + \\bf{\\beta}'\\bf{x} $$\n",
    "\n",
    "- And if we call $f(x) = \\beta_0 + \\bf{\\beta}'\\bf{x}$, we may redefine the bounds of our classifier.\n",
    "\n",
    "- For instance, if we want quadratic bounds:\n",
    "\n",
    "$$ f(x) = \\beta_0 + \\bf{\\beta}_1'\\bf{x} + \\bf{\\beta}_2\\bf{x}'I\\bf{x} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e76b1d2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Support Vector Machines\n",
    "\n",
    "- And to generalize:\n",
    "\n",
    "$$ f(x) = \\beta_0 + \\sum_i \\alpha_i \\langle \\bf{x},x_i \\rangle $$\n",
    "\n",
    "- Where $\\langle \\bf{x},x_i \\rangle$ is the [inner product](https://en.wikipedia.org/wiki/Inner_product_space) of two observations.\n",
    "\n",
    "- The inner product is *sort like the multiplication operation for vectors*.\n",
    "\n",
    "- Support vector machines toy with these definitions of inner products."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e2f0c3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Support Vector Machines\n",
    "\n",
    "- The generalization is called the `kernel`.\n",
    "\n",
    "- The **kernel** will be a function that quantifies the similarity between two observations.\n",
    "\n",
    "- *Linear Kernel* (supporting vector classifier):\n",
    "\n",
    "$$ K(x_i, x_i') = \\sum_j x_{ij}x_{i'j} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb5bf74",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Support Vector Machines\n",
    "\n",
    "- *Polynomial Kernel* (d > 1):\n",
    "\n",
    "$$ K(x_i, x_i') = \\left[1 + \\sum_j x_{ij}x_{i'j} \\right]^d $$\n",
    "\n",
    "- *Radial Kernel*:\n",
    "\n",
    "$$ K(x_i, x_i') = \\exp\\left[-\\gamma\\sum_j (x_{ij} - x_{i'j})^2 \\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242428cc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Support Vector Machines\n",
    "\n",
    "Poly Kernel ($d = 3$) and Radial Kernel:\n",
    "\n",
    "![img](../img/svm8.png)\n",
    "\n",
    "- Next class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8eace45",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Questions?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d252e9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# See you next class\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
