{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "243e38f9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# POLI 175 - Lecture 19\n",
    "\n",
    "## Tree-based models I"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4046a13",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Tree-based methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b879690",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Tree-based methods\n",
    "\n",
    "- Tree-based methods consist of segmenting the predictors' space into many regions\n",
    "\n",
    "- Then, use these regions to predict the target variable.\n",
    "    + We use heuristics here, such as the variable's mean in the region.\n",
    "    \n",
    "- This approach is called the `decision tree method`\n",
    "\n",
    "- It, by itself, is terrible. But then many methods improve the efficiency considerably\n",
    "    + At the cost of interpretability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde1bd55",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Decision Trees\n",
    "\n",
    "- Can both be applied to regression and classification.\n",
    "\n",
    "- They look like this:\n",
    "\n",
    "![img](../img/tree1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639cf822",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Decision Trees\n",
    "\n",
    "- This is how they segment the space:\n",
    "\n",
    "![tree2](../img/tree2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231d14c8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Decision Trees\n",
    "\n",
    "- Formally:\n",
    "    1. We divide the predictors' space into distinct and non-overlapping regions $R_1$, $R_2$, $\\cdots$, $R_J$.\n",
    "    2. For all observations within $R_j$, we make the same prediction.\n",
    "    \n",
    "- How to construct the $R_1, R_2, \\cdots, R_J$ space?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731da624",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Decision Trees\n",
    "\n",
    "- We minimize the RSS of the following equation:\n",
    "\n",
    "$$ \\sum_{j=1}^J\\sum_{i \\in R_j} (y_i - \\hat{y}_{R_j})^2 $$\n",
    "\n",
    "- But it is easy to see that solving this is computationally infeasible.\n",
    "    + Multiple variables would amount to multiple regions.\n",
    "    + Many combinations and regions would be possible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8591f613",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Decision Trees\n",
    "\n",
    "- The way we do this is called the `greedy` approach:\n",
    "\n",
    "1. We consider one variable at a time.\n",
    "\n",
    "2. We split it recursively.\n",
    "\n",
    "3. We consider the regions that divide the space into two half-spaces:\n",
    "\n",
    "$$ R_1(j, s) = \\{X | X_j < s\\} \\quad and \\quad R_2(j, s) = \\{X | X_j \\geq s\\} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39388f7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Decision Trees\n",
    "\n",
    "- Minimizing:\n",
    "\n",
    "$$ \\sum_{i: \\ x_i \\in R_1(j,s)} (y_i - \\hat{y}_{R_1})^2 + \\sum_{i: \\ x_i \\in R_2(j,s)} (y_i - \\hat{y}_{R_2})^2$$\n",
    "\n",
    "- With $\\hat{y}_{R_j}$ the mean of $y$ in the $j$-th half-space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deaf8fcb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Decision Trees\n",
    "\n",
    "- This could be the optimal, but never the greedy approach:\n",
    "\n",
    "![tree3](../img/tree3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed46502",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Decision Trees\n",
    "\n",
    "- This represents the greedy approach:\n",
    "\n",
    "![tree3](../img/tree4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60396a32",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Decision Trees\n",
    "\n",
    "- And this is the levels / the regression tree:\n",
    "\n",
    "![tree5](../img/tree5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6033e7aa",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Decision Trees\n",
    "\n",
    "- This approach will likely overfit the data. We need them to `prune` our tree a bit.\n",
    "\n",
    "- This involves removing parts of our tree that are not being helpful in the testing set predictions.\n",
    "\n",
    "- One heuristic is to use the size of the tree to penalize it. This is called `cost complexity pruning`\n",
    "\n",
    "For all $T \\subset T_0$:\n",
    "\n",
    "$$ \\sum_{m=1}^{|T|} \\sum_{i: \\ x_i \\in R_m} (y_i - \\hat{y}_{R_m})^2 + \\alpha |T| $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101c58bb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Decision Trees\n",
    "\n",
    "**Regression Tree Algorithm:**\n",
    "\n",
    "1. Use recursive binary split (greedy approach) to grow a large tree on the training data. Stop when the terminal node reaches fewer than some threshold minimum number of observations.\n",
    "\n",
    "2. Apply the cost-complexity pruning to the large tree to obtain a sequence of trees $T_1 \\subset T_2 \\subset \\cdots \\subset T_0$, as a function of $\\alpha$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716eec95",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Decision Trees\n",
    "\n",
    "**Regression Tree Algorithm:**\n",
    "\n",
    "3. Use K-fold cross-validation to choose $\\alpha$, i.e., for each $k \\in \\{1, 2, \\cdots, K\\}$:\n",
    "    - Repeat steps 1 and 2 on all but $k$th fold of training data\n",
    "    - Compute the MSE in the left-out fold\n",
    "    - Pick $\\alpha$ that minimizes the MSE.\n",
    "\n",
    "4. Fit the corresponding tree for the optimal $\\alpha$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24850020",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Decision Trees\n",
    "\n",
    "![tree6](../img/tree6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83a2ec1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Classification Trees\n",
    "\n",
    "- Very similar, but you change the prediction and the statistic we look at.\n",
    "\n",
    "1. We predict that all classes belong to the `most commonly occurring class` in the data.\n",
    "\n",
    "2. We look at the `classification error rates` to grow our trees."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e76b1d2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Classification Trees\n",
    "\n",
    "- Let $\\hat{p}_{mk}$ the proportion of cases in the $m$-th region that belong to the $k$-th class.\n",
    "\n",
    "- The error rate is:\n",
    "\n",
    "$$ E = 1 - \\max_k (\\hat{p}_{mk}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e2f0c3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Classification Trees\n",
    "\n",
    "- But, the classification error is sensitive to the size of the tree. Therefore, it is preferable to also look at the following:\n",
    "\n",
    "1. The `Gini index`:\n",
    "\n",
    "$$ G = \\sum_k \\hat{p}_{mk}(1 - \\hat{p}_{mk}) $$\n",
    "\n",
    "2. Or the `Entropy` level:\n",
    "    \n",
    "$$ D = - \\sum_k \\max_k \\hat{p}_{mk}\\log(\\hat{p}_{mk}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb5bf74",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Classification Trees\n",
    "\n",
    "- [Gini index](https://en.wikipedia.org/wiki/Gini_coefficient): measure of *node purity*: \n",
    "\n",
    "    - Small values indicate that all $\\hat{p}_{mk}$ are either close to zero or one.\n",
    "\n",
    "- [Entropy](https://en.wikipedia.org/wiki/Entropy):\n",
    "\n",
    "    - Easy to see that: $0 \\leq -\\hat{p}_{mk}\\log(\\hat{p}_{mk})$\n",
    "    - It will take values close to zero if $\\hat{p}_{mk}$ is close to either zero or one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242428cc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Classification Trees\n",
    "\n",
    "- Feel free to use the measurement that you prefer.\n",
    "\n",
    "- But, minimizing error rates could be preferable when classification accuracy is the target.\n",
    "\n",
    "- But please cross-validate your analysis to pick the best parameters/tree size."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779a4ebe",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Classification Trees\n",
    "\n",
    "Classification in the book's heart disease data (before pruning):\n",
    "\n",
    "![tree7](../img/tree7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56912cd6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Classification Trees\n",
    "\n",
    "Classification in the book's heart disease data (after pruning):\n",
    "\n",
    "![tree8](../img/tree8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655eaa21",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Decision Trees x Linear Models\n",
    "\n",
    "- Linear regression:\n",
    "\n",
    "$$ f(X) \\ = \\ \\beta_0 + \\sum_{j=1}^p X_j\\beta_j $$\n",
    "\n",
    "- Decision Trees:\n",
    "\n",
    "$$ f(X) \\ = \\ \\sum_{m=1}^M c_m \\cdot \\mathbb{1}(X \\in R_m) $$\n",
    "\n",
    "- Which model is better? No easy answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d203cdb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Decision Trees x Linear Models\n",
    "\n",
    "Regression is better:\n",
    "\n",
    "![tree9](../img/tree9.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2883eb16",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Decision Trees x Linear Models\n",
    "\n",
    "Decision tree is better:\n",
    "\n",
    "![tree10](../img/tree10.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d850f88",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Decision Trees\n",
    "\n",
    "- **Pros**:\n",
    "\n",
    "    1. Easy to explain.\n",
    "\n",
    "    2. Some argue that it mirrors human decision-making.\n",
    "\n",
    "    3. Allow for graphical display (kind of pretty, if you ask me...).\n",
    "\n",
    "    4. Easily handle qualitative predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5a1f25",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Decision Trees\n",
    "\n",
    "- **Cons**:\n",
    "\n",
    "    1. Do not have the same level of accuracy as some predictive regression models.\n",
    "    \n",
    "    2. Can be *non-robust*: small changes in data make up to significant changes in final estimation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8eace45",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Questions?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d252e9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# See you next class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b55c5cf0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'get_scorer_names' from 'sklearn.metrics' (/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/57/5cl1vf6549d34n3t9xpfz0fr0000gn/T/ipykernel_26071/3666357372.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnaive_bayes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGaussianNB\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiscriminant_analysis\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLinearDiscriminantAnalysis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mQuadraticDiscriminantAnalysis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassification_report\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecision_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_scorer_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean_squared_error\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr2_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLeaveOneOut\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mKFold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minspection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDecisionBoundaryDisplay\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'get_scorer_names' from 'sklearn.metrics' (/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/__init__.py)"
     ]
    }
   ],
   "source": [
    "## Loading the relevant packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Plotting things:\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Look at our friend here to help with GAM\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.gam.api import GLMGam, BSplines\n",
    "\n",
    "# Loading scikit learn relevant packages (note our new friends!)\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression, Ridge, Lasso\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "from sklearn.metrics import confusion_matrix, classification_report, precision_score, get_scorer_names, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split, LeaveOneOut, cross_val_score, KFold, GridSearchCV\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures, SplineTransformer\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56729f6a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "## Education Expenditure Dataset\n",
    "educ = pd.read_csv('https://raw.githubusercontent.com/umbertomig/POLI175public/main/data/educexp.csv')\n",
    "educ.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e7e928",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "## Decision Trees\n",
    "y = educ['education']\n",
    "X = educ[['income', 'young', 'urban']]\n",
    "X_zed = StandardScaler().set_output(transform = 'pandas').fit_transform(X)\n",
    "X_zed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec9fa11",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "## Creating the model\n",
    "tree = DecisionTreeClassifier()\n",
    "\n",
    "## Split sample:\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=12345)\n",
    "\n",
    "## Training the model\n",
    "tree = tree.fit(X_train,y_train)\n",
    "\n",
    "## Predicting outcomes\n",
    "y_pred = tree.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693cf252",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "## Check the Decision Tree\n",
    "plot_tree(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83dfab3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "## Chile Dataset\n",
    "chile = pd.read_csv('https://raw.githubusercontent.com/umbertomig/POLI175public/main/data/chilesurvey.csv')\n",
    "chile.head()"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
